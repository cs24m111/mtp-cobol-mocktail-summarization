#!/usr/bin/env python
"""
End-to-end pipeline for ALL projects and ALL .cbl files:

For each COBOL file under --projects-root:

1) Static / graph extraction  (AST, CFG, BR rules, BRR, DFG, PDG)
   - We REUSE outputs generated by run_all_projects_static.py.
     This script no longer calls an extractor itself.

2) BR + Program index + BR_REP:
   - summarizer.build_br_json_from_dot
   - summarizer.program_index
   - summarizer.br_representation

3) Mocktail prompts:
   - summarizer.run_summarization.build_prompts_for_program
     (modes from summarizer.mocktail_config)

4) LLM (Ollama) summarisation:
   - Rule-level summaries for each mocktail mode
   - File-level summary per program + mode

5) Evaluation:
   - Per-file metrics vs references:
       * first from CSV (e.g. file_level_reference_dataset.csv)
       * fallback to data/human_references_generated/<proj>/<file>.txt
   - Per-project averages
   - Global averages per mode

Usage example (from COBREX-CLI root):

    python mtp_full_pipeline_all_projects.py \\
        --projects-root data/project_clean \\
        --output-root  output \\
        --ref-csv      data/file_level_reference_dataset.csv \\
        --human-ref-root data/human_references_generated \\
        --model       llama3.1

"""

from __future__ import annotations

import argparse
import csv
import subprocess
from collections import defaultdict
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

from ollama_utils import generate_text
from summarizer.mocktail_config import DEFAULT_MOCKTAIL_MODES
from summarizer.run_summarization import build_prompts_for_program


# ---------------------------------------------------------------------------
# 0) Discovery helpers
# ---------------------------------------------------------------------------

def discover_cobol_files(projects_root: Path) -> Iterable[Tuple[str, Path]]:
    """
    Yield (project_name, cbl_path) for all .cbl files under projects_root.

    Works for two layouts:
      1) projects_root/<project>/<file>.cbl
      2) projects_root/<file>.cbl   (single-project root)
    """
    for cbl_path in projects_root.rglob("*.cbl"):
        rel = cbl_path.relative_to(projects_root)
        parts = rel.parts
        if len(parts) == 1:
            # Single project: use directory name as project
            project = projects_root.name
        else:
            project = parts[0]
        yield project, cbl_path



def program_id_for_file(project: str, cbl_path: Path, projects_root: Path) -> str:
    """
    Program ID = COBOL file name without extension, e.g.:

      data/project_clean/IBM_example-health-apis/HCAPDB01.cbl
      -> prog = 'HCAPDB01'
    """
    return cbl_path.stem


# ---------------------------------------------------------------------------
# 1) Static pipeline (AST/CFG/DFG/PDG/BR/BRR)
# ---------------------------------------------------------------------------

def ensure_static_pipeline_for_program(
    prog: str,
    cobol_file: Path,
    prog_out_dir: Path,
) -> bool:
    """
    Ensure that the low-level static outputs exist for this program:
      - AST / CFG / DFG / PDG / BR / BRR
      - (at least) ProgramIndex + BR_REP prerequisites

    We **do not** run any extractor here.
    We assume run_all_projects_static.py has already created the static
    outputs under prog_out_dir (e.g. Rules/, CFG/, DFG/, PDG/, ...).

    Returns:
      True  -> static outputs look OK, continue pipeline
      False -> static outputs missing; caller should SKIP this file
    """
    index_path = prog_out_dir / "INDEX" / f"ProgramIndex_{prog}.json"
    br_rep_dir = prog_out_dir / "BR_REP"
    rules_dir = prog_out_dir / "Rules"

    # If ProgramIndex + BR_REP already exist, we’re fully good.
    if index_path.is_file() and br_rep_dir.is_dir() and any(br_rep_dir.glob("BR_*.json")):
        print(f"[STATIC] Reusing existing static + BR outputs for {prog}")
        return True

    # If we at least have Rules/, we can *try* to build BR/INDEX/BR_REP.
    if rules_dir.is_dir():
        print(f"[STATIC] Using pre-built static outputs for {prog} at {prog_out_dir}")
        return True

    # Otherwise, static step clearly failed for this program → skip it.
    print(f"[STATIC][WARN] Missing static outputs for {prog} at {prog_out_dir}")
    print("  → Expected this directory to be created by run_all_projects_static.py")
    print("  → Skipping this file and continuing with others.")
    return False


# ---------------------------------------------------------------------------
# 2) BR JSON + ProgramIndex + BR_REP
# ---------------------------------------------------------------------------

def run_br_and_index_pipeline(prog: str, prog_out_dir: Path) -> bool:
    """
    Run:
      - summarizer.build_br_json_from_dot
      - summarizer.program_index
      - summarizer.br_representation

    We assume run_all_projects_static.py has already been run and that
    Rules/ contains rule graphs (files named like 'rule_1', 'BRR_*', etc.).

    Returns:
      True  -> BR JSON + ProgramIndex + BR_REP successfully created
      False -> something missing / failed; caller should skip this file
    """
    rules_dir = prog_out_dir / "Rules"

    if not rules_dir.is_dir():
        print(f"[BR][WARN] Rules dir not found for {prog}: {rules_dir}")
        print("  → Skipping this file and continuing.")
        return False

    # Consider any non-PDF file inside Rules/ as a rule graph.
    rule_files = [
        p for p in rules_dir.iterdir()
        if p.is_file() and not p.name.lower().endswith(".pdf")
    ]

    if not rule_files:
        print(f"[BR][WARN] No rule graph files found for {prog} in {rules_dir}")
        print("  → Expected files like 'rule_1', 'BRR_<PROG>', etc.")
        print("  → Skipping this file and continuing.")
        return False

    print(f"[BR] Found {len(rule_files)} rule file(s) for {prog} in {rules_dir}: "
          f"{[p.name for p in rule_files]}")

    br_json_path = prog_out_dir / f"BR_{prog}.json"

    # 1) Build BR JSON
    cmd1 = [
        "python", "-m", "summarizer.build_br_json_from_dot",
        "--prog", prog,
        "--rules-dir", str(rules_dir),
        "--out-json", str(br_json_path),
    ]
    print("[RUN]", " ".join(cmd1))
    try:
        subprocess.check_call(cmd1)
    except subprocess.CalledProcessError as e:
        print(f"[BR][ERR] build_br_json_from_dot failed for {prog}: {e}")
        print("  → Skipping this file and continuing.")
        return False

    # 2) Build ProgramIndex
    cmd2 = [
        "python", "-m", "summarizer.program_index",
        "--prog", prog,
        "--base-dir", str(prog_out_dir),
        "--br-json", str(br_json_path),
    ]
    print("[RUN]", " ".join(cmd2))
    try:
        subprocess.check_call(cmd2)
    except subprocess.CalledProcessError as e:
        print(f"[BR][ERR] program_index failed for {prog}: {e}")
        print("  → Skipping this file and continuing.")
        return False

    # 3) Build BR_REP
    cmd3 = [
        "python", "-m", "summarizer.br_representation",
        "--prog", prog,
        "--base-dir", str(prog_out_dir),
    ]
    print("[RUN]", " ".join(cmd3))
    try:
        subprocess.check_call(cmd3)
    except subprocess.CalledProcessError as e:
        print(f"[BR][ERR] br_representation failed for {prog}: {e}")
        print("  → Skipping this file and continuing.")
        return False

    # If we got here, everything succeeded.
    return True

# ---------------------------------------------------------------------------
# 3) Rule-level and file-level summarisation with Ollama
# ---------------------------------------------------------------------------

def iter_prompt_files(prog_out_dir: Path, prog: str, mode: str) -> Iterable[Tuple[str, Path]]:
    """
    Yield (br_id_safe, prompt_path) for a given program + mode.

    Expects filenames like:
      PROMPT_<prog>_<SAFE_BR_ID>_<MODE>.txt
    under:
      prog_out_dir / BR_PROMPTS / <mode> /
    """
    prompts_dir = prog_out_dir / "BR_PROMPTS" / mode
    if not prompts_dir.is_dir():
        return []

    for path in sorted(prompts_dir.glob(f"PROMPT_{prog}_*_{mode}.txt")):
        stem = path.stem
        parts = stem.split("_")
        # PROMPT, prog, SAFE_BR_ID..., mode
        if len(parts) < 4:
            br_safe = "_".join(parts[2:-1]) or parts[-2]
        else:
            br_safe = "_".join(parts[2:-1])
        yield br_safe, path


def generate_rule_level_summaries_for_program(
    prog: str,
    prog_out_dir: Path,
    modes: List[str],
    model: str,
    cobol_file: Path,
    overwrite: bool = False,
) -> None:
    """
    Generate rule-level summaries.

    Normal path:
      - Use BR_PROMPTS/<mode>/PROMPT_<prog>_*_<MODE>.txt created by
        summarizer.run_summarization.build_prompts_for_program.

    Fallback path (your case for GETLOAN, or any program with no BR_REP/prompts):
      - If NO prompt files are found for a given mode, we:
          * read the original COBOL source file
          * create ONE synthetic prompt that treats the whole program
            as a single 'rule' (id = WHOLE_FILE)
          * then call Ollama on that prompt.

    This guarantees that even programs with no extracted BRs still get:
      - LLM/rule_level/<mode>/RULE_SUMMARY_<prog>_WHOLE_FILE_<mode>.txt
      - LLM/file_level/FILE_SUMMARY_<prog>_<mode>.txt
    """
    base_out = prog_out_dir / "LLM"

    # Read COBOL source once (for fallback use)
    cobol_source: str | None = None
    if cobol_file.is_file():
        try:
            cobol_source = cobol_file.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"[RULE][WARN] Could not read COBOL source for {prog}: {e}")
            cobol_source = None

    for mode in modes:
        # Normal path: prompts created by build_prompts_for_program(...)
        entries = list(iter_prompt_files(prog_out_dir, prog, mode))

        # Fallback: no prompts → create one synthetic prompt from full COBOL
        if not entries and cobol_source:
            prompts_dir = prog_out_dir / "BR_PROMPTS" / mode
            prompts_dir.mkdir(parents=True, exist_ok=True)

            prompt_path = prompts_dir / f"PROMPT_{prog}_WHOLE_FILE_{mode}.txt"

            if not prompt_path.exists() or overwrite:
                prompt_text = f"""
You are an expert COBOL analyst.

Program ID: {prog}
Mocktail mode: {mode}

You are given the FULL COBOL source of this program between <COBOL> tags.

Your tasks:
1. Explain the overall business purpose of the program.
2. Describe the main business rules and what each one does.
3. Highlight key inputs, outputs, and any validation/error-handling logic.
4. Write a concise explanation (about 150–200 words) suitable for a human
   maintainer who is not familiar with COBOL.

<COBOL>
{cobol_source}
</COBOL>

Now write ONLY the explanation.
""".strip()

                prompt_path.write_text(prompt_text, encoding="utf-8")

            entries = [("WHOLE_FILE", prompt_path)]
            print(
                f"[RULE][INFO] No BR prompts for {prog} mode={mode}; "
                f"created 1 synthetic prompt from full COBOL source."
            )

        # Still nothing (e.g. couldn’t read COBOL file)
        if not entries:
            print(f"[RULE][WARN] No prompts for {prog} mode={mode}")
            continue

        mode_out = base_out / "rule_level" / mode
        mode_out.mkdir(parents=True, exist_ok=True)

        print(f"[RULE] {prog} mode={mode} prompts={len(entries)}")

        for br_safe, prompt_path in entries:
            out_path = mode_out / f"RULE_SUMMARY_{prog}_{br_safe}_{mode}.txt"
            if out_path.exists() and not overwrite:
                continue

            prompt = prompt_path.read_text(encoding="utf-8")
            try:
                ans = generate_text(model, prompt)
            except Exception as e:
                print(f"[RULE][ERR] {prog} {br_safe} mode={mode}: {e}")
                continue

            out_path.write_text(ans, encoding="utf-8")



def build_file_level_prompt(
    prog: str,
    summaries_dir: Path,
) -> str:
    """
    Combine all rule-level summaries into one file-level prompt.
    """
    parts: List[str] = []

    entries = sorted(summaries_dir.glob(f"RULE_SUMMARY_{prog}_*.txt"))
    for path in entries:
        stem = path.stem
        bits = stem.split("_")
        # RULE_SUMMARY, prog, SAFE_BR_ID..., mode
        if len(bits) < 4:
            br_id = "_".join(bits[2:-1]) or bits[-2]
        else:
            br_id = "_".join(bits[2:-1])

        summary = path.read_text(encoding="utf-8").strip()
        parts.append(f"<Rule id=\"{br_id}\">\n{summary}\n</Rule>")

    rules_block = "\n\n".join(parts)

    header = f"""
You are a Text Processing Agent that merges COBOL business-rule
explanations into a single, coherent file-level explanation.

Program ID: {prog}

You are given a list of rule-level explanations (one per business rule)
enclosed in <Rule> tags.

Your task:
1. Produce ONE clear, human-readable explanation of the ENTIRE COBOL file.
2. Cover the overall business purpose of the file.
3. Summarise the major responsibilities of each rule and how they interact.
4. Highlight any important validation or error-handling logic.
5. Write 2–4 short paragraphs (150–250 words in total).

Do NOT repeat the <Rule> texts verbatim; synthesise them.
""".strip()

    return header + "\n\n<Rules>\n" + rules_block + "\n</Rules>\n\n" + \
        "Now write ONLY the file-level explanation."


def generate_file_level_summaries_for_program(
    prog: str,
    prog_out_dir: Path,
    modes: List[str],
    model: str,
    overwrite: bool = False,
) -> None:
    llm_root = prog_out_dir / "LLM"
    for mode in modes:
        rule_dir = llm_root / "rule_level" / mode
        if not rule_dir.is_dir():
            print(f"[FILE][WARN] No rule summaries for {prog} mode={mode}")
            continue

        file_out_dir = llm_root / "file_level"
        file_out_dir.mkdir(parents=True, exist_ok=True)

        out_path = file_out_dir / f"FILE_SUMMARY_{prog}_{mode}.txt"
        if out_path.exists() and not overwrite:
            continue

        prompt = build_file_level_prompt(prog, rule_dir)
        try:
            ans = generate_text(model, prompt)
        except Exception as e:
            print(f"[FILE][ERR] {prog} mode={mode}: {e}")
            continue

        out_path.write_text(ans, encoding="utf-8")
        print(f"[FILE] {prog} mode={mode} -> {out_path}")


# ---------------------------------------------------------------------------
# 4) References (CSV + human_references_generated)
# ---------------------------------------------------------------------------

def load_references_from_csv(csv_path: Path) -> Dict[str, str]:
    """
    Load human / gold references from a CSV.

    We try to be flexible with column names:
      - path column:  "file", "file_path", "relative_path", "cbl_path", "file path"
      - reference column: "reference", "reference data", "refined summary2"

    For each CSV row, we register multiple possible keys so that they can
    match how we address files in the code:

        raw path from CSV, e.g. "data/project_clean/IBM_example-health-apis/HCAPDB01.cbl"
        "<project>/<file>.cbl", e.g. "IBM_example-health-apis/HCAPDB01.cbl"
        just "HCAPDB01.cbl"
    """
    print(f"[INFO] Loading referenced files from: {csv_path}")
    if not csv_path.exists():
        print(f"[WARN] Reference CSV not found: {csv_path}")
        return {}

    import pandas as pd

    df = pd.read_csv(csv_path)

    # Strip whitespace from column names (your CSV has "file path")
    df.columns = [c.strip() for c in df.columns]

    possible_path_cols = ["file", "file_path", "relative_path", "cbl_path", "file path"]
    path_col = None
    for col in possible_path_cols:
        if col in df.columns:
            path_col = col
            break

    if path_col is None:
        raise ValueError(
            f"Cannot find path column in {csv_path}; expected one of {possible_path_cols}, got {list(df.columns)}"
        )

    # Normalize paths (remove spaces, unify slashes)
    df[path_col] = df[path_col].astype(str).str.strip()
    df[path_col] = df[path_col].str.replace("\\", "/", regex=False)

    # Try to detect which column has the actual reference text
    possible_ref_cols = ["reference", "reference data", "refined summary2"]
    ref_col = None
    for col in possible_ref_cols:
        if col in df.columns:
            ref_col = col
            break

    if ref_col is None:
        raise ValueError(
            f"Cannot find reference text column in {csv_path}; expected one of {possible_ref_cols}, got {list(df.columns)}"
        )

    refs: Dict[str, str] = {}

    for _, row in df.iterrows():
        path_val = row.get(path_col, "")
        ref_val = row.get(ref_col, "")

        if not isinstance(path_val, str) or not path_val.strip():
            continue
        if not isinstance(ref_val, str) or not ref_val.strip():
            continue

        raw = path_val.strip().replace("\\", "/")
        text = ref_val.strip()
        parts = raw.split("/")

        candidate_keys = set()

        # 1) raw as-is (whatever the CSV had)
        candidate_keys.add(raw)

        # 2) last two components: "<repo>/<file>.cbl"
        if len(parts) >= 2:
            tail2 = "/".join(parts[-2:])
            candidate_keys.add(tail2)

        # 3) just the file name, e.g. "HCAPDB01.cbl"
        candidate_keys.add(parts[-1])

        for key in candidate_keys:
            # don't overwrite an existing reference for same key
            refs.setdefault(key, text)

    print(f"[INFO] Found {len(refs)} unique file keys with references from CSV.")
    return refs


def load_human_generated_refs(human_root: Path) -> Dict[str, str]:
    """
    Load references from data/human_references_generated.

    Assumes structure:
      human_root/<project>/<file>.txt

    We map each .txt to a key:
      "<project>/<file>.cbl"
    (i.e. replace .txt with .cbl)
    """
    refs: Dict[str, str] = {}
    if not human_root.is_dir():
        print(f"[REF][WARN] human_root not found: {human_root}")
        return refs

    for txt_path in human_root.rglob("*.txt"):
        rel = txt_path.relative_to(human_root)
        # change suffix .txt -> .cbl
        rel_cbl = rel.with_suffix(".cbl")
        key = "/".join(rel_cbl.parts)
        text = txt_path.read_text(encoding="utf-8").strip()
        if key not in refs:
            refs[key] = text
    print(f"[REF] Loaded {len(refs)} human-generated references from {human_root}")
    return refs


def merge_references(csv_refs: Dict[str, str], human_refs: Dict[str, str]) -> Dict[str, str]:
    """
    CSV wins; human_refs used as fallback.
    """
    merged = dict(csv_refs)
    for k, v in human_refs.items():
        merged.setdefault(k, v)
    return merged


# ---------------------------------------------------------------------------
# 5) Simple ROUGE-1 F1
# ---------------------------------------------------------------------------

def _tokens(text: str) -> List[str]:
    return [t for t in text.lower().split() if t]


def rouge1_f1(pred: str, ref: str) -> float:
    pt = _tokens(pred)
    rt = _tokens(ref)
    if not pt or not rt:
        return 0.0
    pc = {}
    rc = {}
    for t in pt:
        pc[t] = pc.get(t, 0) + 1
    for t in rt:
        rc[t] = rc.get(t, 0) + 1
    overlap = 0
    for t, c in pc.items():
        overlap += min(c, rc.get(t, 0))
    precision = overlap / len(pt)
    recall = overlap / len(rt)
    if precision + recall == 0:
        return 0.0
    return 2 * precision * recall / (precision + recall)


# ---------------------------------------------------------------------------
# 6) Main driver
# ---------------------------------------------------------------------------

def main(argv: List[str] | None = None) -> None:
    parser = argparse.ArgumentParser(
        description="End-to-end: static + mocktail + Ollama + evaluation for all COBOL files."
    )
    parser.add_argument(
        "--projects-root",
        type=Path,
        required=True,
        help="Root of cleaned projects (contains <project>/*.cbl)",
    )
    parser.add_argument(
        "--output-root",
        type=Path,
        required=True,
        help="Root for COBREX outputs (COBOL_<PROG> dirs)",
    )
    parser.add_argument(
        "--ref-csv",
        type=Path,
        required=True,
        help="CSV with human file-level references",
    )
    parser.add_argument(
        "--human-ref-root",
        type=Path,
        required=True,
        help="Root of human_references_generated",
    )
    parser.add_argument(
        "--modes",
        nargs="+",
        default=list(DEFAULT_MOCKTAIL_MODES),
        help=f"Mocktail modes (default: {list(DEFAULT_MOCKTAIL_MODES)})",
    )
    parser.add_argument(
        "--model",
        default="llama3.1",
        help="Ollama model name (default: llama3.1)",
    )
    parser.add_argument(
        "--overwrite-llm",
        action="store_true",
        help="Re-run Ollama even if summaries already exist.",
    )
    parser.add_argument(
        "--per-file-csv",
        type=Path,
        default=Path("data/eval_per_file.csv"),
        help="Where to write per-file metrics CSV.",
    )
    parser.add_argument(
        "--per-project-csv",
        type=Path,
        default=Path("data/eval_per_project.csv"),
        help="Where to write per-project metrics CSV.",
    )

    args = parser.parse_args(argv)

    # Load references
    csv_refs = load_references_from_csv(args.ref_csv)
    human_refs = load_human_generated_refs(args.human_ref_root)
    all_refs = merge_references(csv_refs, human_refs)

    # For aggregations
    per_file_rows: List[Dict[str, object]] = []
    project_stats: Dict[str, Dict[str, List[float]]] = defaultdict(
        lambda: defaultdict(list)
    )
    global_stats: Dict[str, List[float]] = defaultdict(lambda: [0.0, 0])

    # Main loop
    for project, cbl_path in discover_cobol_files(args.projects_root):
        rel_cbl = cbl_path.relative_to(args.projects_root)
        rel_key = "/".join(rel_cbl.parts)  # e.g. IBM_example-health-apis/HCAPDB01.cbl
        prog = program_id_for_file(project, cbl_path, args.projects_root)

        # ------------------------------------------------------------------
        # Resolve prog_out_dir to match run_all_projects_static.py layout
        # ------------------------------------------------------------------
        candidates = [
            args.output_root / project / f"COBOL_{prog}",
            args.output_root / f"COBOL_{prog}",
            args.output_root / project / prog,
        ]
        prog_out_dir = None
        for cand in candidates:
            if cand.is_dir():
                prog_out_dir = cand
                break
        if prog_out_dir is None:
            prog_out_dir = candidates[0]
            print(
                f"[WARN] No existing output dir found for {prog}. "
                f"Expected one of: {[str(c) for c in candidates]}"
            )

        print("\n==============================")
        print(f"[FILE] Project={project} COBOL={rel_key}  prog={prog}")
        print(f"[FILE] Using prog_out_dir={prog_out_dir}")

        # 1) Static + graphs (reusing outputs from run_all_projects_static.py)
        if not ensure_static_pipeline_for_program(prog, cbl_path, prog_out_dir):
            # Skip this file
            continue

        # 2) BR JSON + ProgramIndex + BR_REP
        #    Best-effort: even if this fails or produces no BR_REP,
        #    we will still fall back to "whole file" summarisation.
        run_br_and_index_pipeline(prog, prog_out_dir)

        # 3) Mocktail prompts (also best-effort; may produce nothing)
        build_prompts_for_program(prog, prog_out_dir, args.modes)

        # 4) Rule-level summaries (with COBOL fallback when no prompts exist)
        generate_rule_level_summaries_for_program(
            prog=prog,
            prog_out_dir=prog_out_dir,
            modes=args.modes,
            model=args.model,
            cobol_file=cbl_path,
            overwrite=args.overwrite_llm,
        )

        # 5) File-level summaries
        generate_file_level_summaries_for_program(
            prog=prog,
            prog_out_dir=prog_out_dir,
            modes=args.modes,
            model=args.model,
            overwrite=args.overwrite_llm,
        )

        # 6) Evaluation (if reference exists)
        ref_text = all_refs.get(rel_key)

        # extra fallback: try just the filename
        if not ref_text:
            basename = cbl_path.name
            ref_text = all_refs.get(basename)

        if not ref_text:
            print(
                f"[EVAL][WARN] No reference found for {rel_key} "
                f"(or {cbl_path.name}), skipping evaluation"
            )
            continue

        llm_root = prog_out_dir / "LLM" / "file_level"
        for mode in args.modes:
            summary_path = llm_root / f"FILE_SUMMARY_{prog}_{mode}.txt"
            if not summary_path.is_file():
                print(f"[EVAL][WARN] Missing file summary for {prog} mode={mode}")
                continue
            pred = summary_path.read_text(encoding="utf-8")
            score = rouge1_f1(pred, ref_text)

            per_file_rows.append(
                {
                    "project": project,
                    "relative_cbl": rel_key,
                    "prog_id": prog,
                    "mode": mode,
                    "rouge1_f1": score,
                }
            )

            project_stats[project][mode].append(score)
            global_stats[mode][0] += score
            global_stats[mode][1] += 1

    # Write per-file CSV
    if per_file_rows:
        args.per_file_csv.parent.mkdir(parents=True, exist_ok=True)
        with args.per_file_csv.open("w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(
                f,
                fieldnames=["project", "relative_cbl", "prog_id", "mode", "rouge1_f1"],
            )
            writer.writeheader()
            writer.writerows(per_file_rows)
        print(f"\n[OUT] Per-file metrics -> {args.per_file_csv}")

    # Per-project summary CSV
    project_rows: List[Dict[str, object]] = []
    for project, mode_dict in project_stats.items():
        for mode, scores in mode_dict.items():
            if not scores:
                continue
            avg = sum(scores) / len(scores)
            project_rows.append(
                {
                    "project": project,
                    "mode": mode,
                    "avg_rouge1_f1": avg,
                    "n_files": len(scores),
                }
            )
    if project_rows:
        args.per_project_csv.parent.mkdir(parents=True, exist_ok=True)
        with args.per_project_csv.open("w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(
                f,
                fieldnames=["project", "mode", "avg_rouge1_f1", "n_files"],
            )
            writer.writeheader()
            writer.writerows(project_rows)
        print(f"[OUT] Per-project metrics -> {args.per_project_csv}")

    # Global aggregates
    if global_stats:
        print("\n[GLOBAL] Averages over ALL projects/files:")
        for mode, (total, count) in global_stats.items():
            avg = total / count if count else 0.0
            print(f"  {mode:20s}: {avg:0.4f}  (n={count})")


if __name__ == "__main__":
    main()
